```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import train_test_split
from sklearn import metrics
import statsmodels.formula.api as smf
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
#1. Read `yelp.csv` into a DataFrame.
yelp_ratings = pd.read_csv('yelp.csv')
* **Alternative:** Construct this DataFrame yourself from `yelp.json`. This involves reading the data into Python, decoding the JSON, converting it to a DataFrame, and adding individual columns for each of the vote types.
#2. Explore the relationship between each of the vote types (cool/useful/funny) and the number of stars.
sns.pairplot(yelp_ratings)
#3. Define cool/useful/funny as the features, and stars as the response.
feature_cols = ['cool','useful','funny']
X = yelp_ratings[feature_cols]
y = yelp_ratings.stars
#4. Fit a linear regression model and interpret the coefficients. Do the coefficients make intuitive sense to you? Explore the Yelp website to see if you detect similar trends.
linreg = LinearRegression()
linreg.fit(X, y)
linreg.intercept_ #cool = .2744, useful = -.1475, funny = -.1357  
linreg.coef_ #y-intercept = 3.8399
lm = smf.ols(formula='stars ~ cool + useful + funny', data=yelp_ratings).fit()
lm.params
lm.pvalues
lm.rsquared
#looking at the scatter matrix, there appears to be correlation among features
#but no real correlation between the features and the response, R-sqaured value is .04
#5. Evaluate the model by splitting it into training and testing sets and computing the RMSE. Does the RMSE make intuitive sense to you?
def train_test_rmse(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
    linreg = LinearRegression()
    linreg.fit(X_train, y_train)
    y_pred = linreg.predict(X_test)
    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))
train_test_rmse(X,y) #1.184; 
#RMSE makes sense given the range of the response
#6. Try removing some of the features and see if the RMSE improves.
feature_cols = ['useful', 'funny'] #1.21
feature_cols = ['cool', 'funny'] #1.19
feature_cols = ['useful'] #1.21
X = yelp_ratings[feature_cols]
#RMSE does not really improve.  
#7. **Bonus:** Think of some new features you could create from the existing data that might be predictive of the response. (This is called "feature engineering".) Figure out how to create those features in Pandas, add them to your model, and see if the RMSE improves.
exclamation_cnt = []
for rating in yelp_ratings.text:
        exclamation_cnt.append(rating.count('!'))
yelp_ratings['exclamation_cnt']=exclamation_cnt
feature_cols = ['exclamation_cnt']
X = yelp_ratings[feature_cols]
def train_test_rmse(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
    linreg = LinearRegression()
    linreg.fit(X_train, y_train)
    y_pred = linreg.predict(X_test)
    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))
train_test_rmse(X,y)
#I thought the number of "!" is a good predictor of star rating (positive correlation), and RMSE would improve; however, RMSE is 1.20
#8. **Bonus:** Compare your best RMSE on testing set with the RMSE for the "null model", which is the model that ignores all features and simply predicts the mean rating in the training set for all observations in the testing set.
#9. **Bonus:** Instead of treating this as a regression problem, treat it as a classification problem and see what testing accuracy you can achieve with KNN.
#10. **Bonus:** Figure out how to use linear regression for classification, and compare its classification accuracy to KNN
```