###Homework 14###
##Mike Yea##

```python
import pandas as pd
#1. Read `yelp.csv` into a DataFrame.
yelp = pd.read_csv('yelp.csv')
#2. Create a new DataFrame that only contains the 5-star and 1-star reviews.
yelp = yelp[(yelp.stars == 5) | (yelp.stars ==1)]
#3. Split the new DataFrame into training and testing sets, using the review text as the feature and the star rating as the response.
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(yelp.text, yelp.stars, random_state=1)
#4. Use CountVectorizer to create document-term matrices from X_train and X_test.
#    * **Hint:** You will have to tell CountVectorizer to ignore any decoding errors.
from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer(decode_error='ignore')
train_dtm = vect.fit_transform(X_train)
test_dtm = vect.transform(X_test)
#5. Use Naive Bayes to predict the star rating for reviews in the testing set, and calculate the accuracy.
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(train_dtm, y_train)
y_pred_class = nb.predict(test_dtm)
from sklearn import metrics
metrics.accuracy_score(y_test, y_pred_class) #.9188
#6. Calculate the AUC.
#    * **Hint 1:** Make sure to pass the predicted probabilities to `roc_auc_score`, not the predicted classes.
#    * **Hint 2:** `roc_auc_score` will get confused if y_test contains fives and ones, so you will need to create a new object that contains ones and zeros instead.
#new dataframe that has 0's and 1's
import numpy as np
replace = {1:0, 5:1}
mp = np.arange(0,6)
mp[replace.keys()] = replace.values()
y_test_new = mp[y_test]
#Calculate AUC
y_pred_prob = nb.predict_proba(test_dtm)[:, 1]
metrics.roc_auc_score(y_test_new, y_pred_prob) #.9404
#7. Plot the ROC curve.
import matplotlib.pyplot as plt
fpr, tpr, thresholds = metrics.roc_curve(y_test_new, y_pred_prob)
plt.plot(fpr, tpr)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
#8. Print the confusion matrix, and calculate the sensitivity and specificity.
metrics.confusion_matrix(y_test_new, y_pred_class)
#Sensitivity = 813/838 = .97
#Specificity = 126/184 = .68 *did I read this right?
#9. Browse through the review text for some of the false positives and false negatives. Based on your knowledge of how Naive Bayes works, do you have any theories about why the model is incorrectly classifying these reviews?
y_test[y_test < y_pred_class] #false positive
y_test[y_test > y_pred_class] #false negative
#a few reviews that have words that the classfier does not know but completely changes the nature of the rating
#10. Let's pretend that you want to balance sensitivity and specificity. You can achieve this by changing the threshold for predicting a 5-star review. What threshold approximately balances sensitivity and specificity?
#Setting the threshold where one achieves .9 sensitivity and .84 (while looking at the  left-most edge of the ROC curve)
thresholds[251] #.995
tpr[251] #.901
fpr[251] #.158
```