{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Predicting User Engagement in Corporate Collaboration Network\n",
    "\n",
    "by Mike Yea\n",
    "DAT7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Background\n",
    "\n",
    "In 2012, an opt-in, web-based (and mobile-enabled) collaboration network was launched at my organization, a 90,000-employee federal agency.  To date, this tool is the only one of its kind that spans across the entire agency.  While initial roll-out and user adoption were impressive (50 percent of users joined in the first 6 months), the growth rate of the network has slowed.  Among chief complaints from users is a high number of unresponded \"messages\" or posts.  Without active collaboration, the network--which is designed to foster the breaking down of organizational silos and link a geographically distributed workforce-will become a shell of its former self. To prevent such an outcome, my colleagues and I are interested in launching an user engagement campaign to induce a \"lift\" in user engagement.  Rather than innundating users and potential users with mass marketing material, we would like to engage users in an informed way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Problem Statement\n",
    "\n",
    "**Can I predict a “lift” in user engagement (i.e., replies to public messages) from message attributes (e.g., length of the message, message posed in the form of a question, presence of attachments and hyperlink, key words, message tone/sentiment, message text, information about message poster)?**    \n",
    "\n",
    "My hypotheses are:\n",
    "1. Message content or references cited in the message are correlated positively with the number of replies.\n",
    "2. Message poster's tenure or role within the organization are correlated with the number of replies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Data\n",
    "\n",
    "###3.1 Data Import\n",
    "\n",
    "The collaboration network has approximately 3 years of data, and the data can be exported via a web interface.  I imported both message (messages.csv) and user profile (users.csv) data for the last one year.  I used Pandas to extract data, but encountered some encoding errors. By importing and using the \"sys\" library, I was able to parse the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "messages = pd.read_csv('Messages.csv', na_filter=False)\n",
    "messages.shape\n",
    "messages.describe()\n",
    "messages.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2 Data Pre-Processing\n",
    "\n",
    "Because the purpose of this study is learning about the public interaction of users, 1) I removed private messages and messages posted in private groups; and 2) stored them in a data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages.in_private_group.sum()\n",
    "messages.in_private_conversation.sum()\n",
    "public_msgs = messages[(messages.in_private_group == False) & (messages.in_private_conversation == False)]\n",
    "public_msgs = public_msgs.drop(['in_private_group', 'in_private_conversation'], axis=1)\n",
    "public_msgs = public_msgs.replace('', 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fields are as follows (filtered to include fields used in the project):\n",
    "\n",
    "**messages-**\n",
    "    id: unique identifier for each message\n",
    "    replied_to_id: id of the message to which the subject message is replying; blank if top-\n",
    "    level message\n",
    "    thread_id: id of the top-level message \n",
    "    group_id: id of the network group\n",
    "    group_name: name of the group\n",
    "    participants: user id(s) of participants in the thread\n",
    "    in_private_group: whether or not the message is posted to a private group (boolean)\n",
    "    in_private_conversation: whether or not the message is private (boolean) \n",
    "    sender_id: id of the message's author\n",
    "    body: message text\n",
    "    attachments: internal identifier\n",
    "    created_at: DTG when the message was posted\n",
    "\n",
    "**users-**\n",
    "    id: unique identifier for each user\n",
    "    job_title: user entered position description\n",
    "    joined_at: when the user joined the network\n",
    "    state: whether the user is active or not (boolean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.3 Response Variable\n",
    "\n",
    "Additionally, messages in messages.csv are not normalized; both top-level (or headline) messages and replies in the thread are stored in the same table.  Since I primarily am interested how the top-level messages induce user engagement (i.e., reply to the initial message), I created a separate data frame that only contains top-level messages and added to that data frame a series that contained the number of replies to each top-level message: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Create response variable column in a dataframe\n",
    "'''\n",
    "#1.1 Initialize a dataframe\n",
    "y_data = pd.DataFrame()\n",
    "y_data = y_data.fillna(0)\n",
    "#1.2 Add number of replies to each original message\n",
    "#1.2.1 Populate dataframe with top-level message ID\n",
    "top_msg_id = []\n",
    "for index, row in public_msgs.iterrows():\n",
    "    if row.id == row.thread_id:\n",
    "        top_msg_id.append(row.id)\n",
    "y_data['id'] = top_msg_id\n",
    "#1.2.2 Create a function that returns the number of replies to a given top-level message ID\n",
    "def get_reply_counts(id):    \n",
    "    public_msgs['num_reply'] = public_msgs.replied_to_id.str.contains(str(id)).astype(int)\n",
    "    return public_msgs['num_reply'].sum()    \n",
    "    \n",
    "#1.2.3 Determine the number of replies to each top-level message and store it to the dataframe\n",
    "cnt_replies = []\n",
    "for row in y_data.id:\n",
    "    cnt_replies.append(get_reply_counts(row))\n",
    "y_data['num_replies'] = cnt_replies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by several active users, a vast majority, 80.02 percent, of messages in the sample data go unanswered (amazingly enough, this number has hovered around 80% throughout the **history of the collaboration network**).  A histogram of the number of replies is depicted as follows:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hist_num.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Feature Analysis and Selection\n",
    "\n",
    "###4.1 Feature Engineering\n",
    "\n",
    "Given the volume of unstructured data, transforming the body of the message into a document term matrix (DTM) appear to be a good idea (user-entered job title also was transformed into a DTM).  Additionally, there are 8 features that I hypothesized would correlate with the response variable:\n",
    "\n",
    "    1. message posted in a group (binary)\n",
    "    2. attachments (binary)\n",
    "    3. length of message (continuous)\n",
    "    4. hyperlinks (binary)\n",
    "    5. message tone/sentiment (index between -1 and 1)\n",
    "    6. message posed as a question (binary)\n",
    "    7. number of key words observed over time (\"experience\", \"opportunity\", and \"interest\") that appear to draw user engagement (continuous)\n",
    "    8. message poster's tenure in the collaboration network (number of days; continuous)\n",
    "    \n",
    "A number of different approaches was used to engineer the above features.  Shown below as an example is the key word feature (#7) using Regular Expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2. Add features to the dataframe\n",
    "'''\n",
    "\n",
    "#2.7 Key words [\"experience\", \"opportunity\", \"interest\"] use apply(key_word_search)\n",
    "import re\n",
    "def search_key_words(text):\n",
    "    return len(re.findall(r\"(experience|opportunity|interest)\", text))\n",
    "public_msgs['has_key_word'] = public_msgs.body.apply(search_key_words)\n",
    "df = public_msgs[['id','has_key_word']]\n",
    "y_data = pd.merge(y_data,df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.2 Feature Selection\n",
    "\n",
    "By plotting a scatter plot (response vs. each feature)chart and adding a regression line, it would be possible to determine what features appear to be correlated strongly with the response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"pair_wise_1.png\"> \n",
    "<img src=\"pair_wise_2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the linear regression model, it seems all the features, individually and collectively, have limited explanatory power, and the linear model does not appear to be a good fit.  The negative slope of the \"message sentiment/tone\" chart was unexpected; I thought this feature and the response would be correlated positively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5. Modeling\n",
    "\n",
    "###5.1 Conversion to a Classifier Model\n",
    "\n",
    "I realized early in the process an OLM regression model would not be a good candidate (R2 value of .05 and RMSE 1.5)  Hence, the continuous response variable was converted to class, making this a classification problem.  This conversion also is useful for the purpose of predicting a particular number of replies, but in a degree of user engagement represented by the classes.  The reponse variable is converted into a 3-class response as follows:\n",
    "    \n",
    "    **2**: more than one reply (approximately 10% of all responses)\n",
    "    **1**: one reply (approximately 10% of all responses)\n",
    "    **0**: no reply (approxmiately 80% of all reponses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "4. Logistic Regression using DTM of body of the message as a feature and other features\n",
    "'''\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy as sp\n",
    "#4.1 Convert the response variable to classes\n",
    "def convert_to_class(num_replies):\n",
    "    num_replies_class = int    \n",
    "    if num_replies > 1:\n",
    "        num_replies_class = 2\n",
    "    elif num_replies == 1:\n",
    "        num_replies_class = 1\n",
    "    else:\n",
    "        num_replies_class = 0\n",
    "    return num_replies_class\n",
    "y_data['num_replies_class'] = y_data.num_replies.apply(convert_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.2 Logistic Regression\n",
    "There were three models considered for evaluation at the outset: 1) logistic regresson model using message body DTM; 2) logistic regression model using job title of the message author DTM; and 3) logistic regression model using both DTMs and a sparse matrix representing the 8 features developed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4.2 Add the body of the message to the dataframe\n",
    "df = public_msgs[['id','body']]\n",
    "y_data = pd.merge(y_data,df)\n",
    "#4.3 Split the new DataFrame into training and testing sets\n",
    "feature_cols = ['body', 'job_title', 'in_group', 'has_attach','msg_len', 'has_qm', 'has_key_word', 'author_age']\n",
    "X = y_data[feature_cols]\n",
    "y = y_data['num_replies_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "#4.4 Use CountVectorizer with body of the message only\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "train_dtm = vect.fit_transform(X_train[:, 0])\n",
    "test_dtm = vect.transform(X_test[:, 0])\n",
    "#4.4b Use CountVectorizer with job_title of the author only\n",
    "train_dtm_jt = vect.fit_transform(X_train[:, 1])\n",
    "test_dtm_jt = vect.transform(X_test[:, 1])\n",
    "#4.5 Cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(X_train[:, 2:].astype(float))\n",
    "#4.6 Combine sparse matrices\n",
    "train_dtm_extra = sp.sparse.hstack((train_dtm, train_dtm_jt, extra))\n",
    "#4.7 Repeat for testing set\n",
    "extra = sp.sparse.csr_matrix(X_test[:, 2:].astype(float))\n",
    "test_dtm_extra = sp.sparse.hstack((test_dtm, test_dtm_jt, extra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.3 Model Evaluation\n",
    "\n",
    "I predicted the null accuracy is around .7 (weighted average of (2,1)=.2 * 1.5 = .3; 1-.3 = .7), and the calculation (.714) supports my prediction.  Since the ROC curve and UAC for a multi-class problem is not supported, class prediction accuracy is the primary evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4.8 Use logistic regression with the body of the message (**Model 1**)\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.716\n",
    "#4.8b Use logistic regression with thre job title of the author (**Model 2**)\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(train_dtm_jt, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm_jt)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.785\n",
    "#4.9 Use logistic regression with all features (**Model 3**)\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm_extra)\n",
    "y_pred_prob = logreg.predict_proba(test_dtm_extra)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.770\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model listed above (logistic regression model using the job title of the author as DTM) has the best performance among the three.  The first model is only marginally better at making class predictions than the null model.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6 Findings and Conclusions\n",
    "\n",
    "###6.1 Findings/Conclusions (Interim until final report)\n",
    "\n",
    "    a. The eight features chosen or engineered are poor predictors of class\n",
    "    b. However, the imbalance between the response of \"0\" and the remaining classes resulted in a high standard (i.e., null accuracy over .7).  I would like to purse some techniques to reduce the impact of class imbalance (OH 8/1)\n",
    "    c. Although not discussed above, I converted the response variable to a binary class problem (1: all messages with the number of replies greater than 0; 0: all messages with no reply).  However, none of the alternatives exceeded the performance of the null model\n",
    "    d. The model using the DTM of the job title (self-populated by the user and is not a mandatory field during user registration) has the best class prediction accuracy (need to look at other features such as @mention in the body of the message)\n",
    "    e. Reject the hypothesis that the body of the message and metadata of the message are predictors of user response to the message\n",
    "    d. Do not reject the hypothesis that the message author's position/title is a strong predictor (office hour: code that summarizes top tokens of job_title DTM)\n",
    "    \n",
    "###6.3 Future Work\n",
    "\n",
    "    a. Only about 4% of 18,000 users are \"engaged\" (i.e., post content, comment on other users' content, click on the like button) at any given reporting period.  Approximately, 30% of all users are considered \"lurkers.\"  Lurker data is not available but is obtainable, provided I get approval from the CIO.  Using both active user and lurker data is critical to measuring the true engagement level of users  \n",
    "    b. Understanding what features are positively correlated with the response is critical to the overall aim of the project.  I hope to find general features that can serve as \"levers\" of our marketing arm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
